{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e398c675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2629b39c",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21ad21f",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1d07bb",
   "metadata": {},
   "source": [
    "### 1. Write a python program which searches all the product under a particular product from www.amazon.in. The product to be searched will be taken as input from user. For e.g. If user input is ‘guitar’. Then search for guitars. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9419eee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = input(str(\"Enter the product you want to search for: \"))\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get('https://www.amazon.in/s?k='+search)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b7697c",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d303109c",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee77c05c",
   "metadata": {},
   "source": [
    "### 2. In the above question, now scrape the following details of each product listed in first 3 pages of your search results and save it in a data frame and csv. \n",
    "In case if any product has less than 3 pages in search results then scrape all the products available under that product name. Details to be scraped are: \"Brand Name\", \"Name of the Product\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\" and “Product URL”. In case, if any of the details are missing for any of the product then replace it by “-“. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cc8ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_name=[]\n",
    "product_name=[]\n",
    "price_tag=[]\n",
    "return_Exchange=[]\n",
    "expected_Delivery=[]\n",
    "availability=[]\n",
    "prod_url=[]\n",
    "\n",
    "\n",
    "#fetching every product url from first three pages\n",
    "for page_count in range(0,3):\n",
    "    time.sleep(5)\n",
    "    #in the first page there were sponsered product in the result which have different class name. So by using CSS_SELECTOR Property i am fetching all the product using , (comma) because OR operator was not working\n",
    "    sections = driver.find_elements(By.CSS_SELECTOR,'.sg-col-4-of-24.sg-col-4-of-12.s-result-item.s-asin.sg-col-4-of-16.AdHolder.sg-col.s-widget-spacing-small.sg-col-4-of-20,.sg-col-4-of-24.sg-col-4-of-12.s-result-item.s-asin.sg-col-4-of-16.sg-col.s-widget-spacing-small.sg-col-4-of-20,.sg-col-4-of-24.sg-col-4-of-12.s-result-item.sg-col-4-of-16.sg-col.sg-col-4-of-20')\n",
    "    time.sleep(2)\n",
    "    for item in sections:\n",
    "        link = item.find_element(By.CSS_SELECTOR,'.a-link-normal.s-underline-text.s-underline-link-text.s-link-style.a-text-normal')\n",
    "        prod_url.append(link.get_attribute('href'))\n",
    "    time.sleep(2)  \n",
    "    try:\n",
    "        driver.find_element(By.CSS_SELECTOR,'.s-pagination-item.s-pagination-next.s-pagination-button.s-pagination-separator').click()\n",
    "    except:\n",
    "        break\n",
    "    \n",
    "    \n",
    "    \n",
    "# we have all the product URL\n",
    "# now fetching the data\n",
    "time.sleep(3)\n",
    "\n",
    "for link in prod_url:\n",
    "    driver.get(link)\n",
    "    \n",
    "    # fetching the brand name\n",
    "    try:\n",
    "        one = driver.find_element(By.ID,\"bylineInfo\").text\n",
    "        brand_name.append(one)\n",
    "    except:\n",
    "        brand_name.append(\"-\")\n",
    "    \n",
    "\n",
    "    # fetching the product name\n",
    "    try:\n",
    "        two = driver.find_element(By.XPATH,'//span[@id=\"productTitle\"]').text\n",
    "        product_name.append(two)\n",
    "    except:\n",
    "        product_name.append(\"-\")\n",
    "    \n",
    "\n",
    "    #fetching the price\n",
    "    try:\n",
    "        three = driver.find_element(By.XPATH,'//span[@class=\"a-price-whole\"]').text\n",
    "        price_tag.append(three)\n",
    "    except:\n",
    "        price_tag.append(\"-\")\n",
    "    \n",
    "\n",
    "    #is product returnable\n",
    "    try:\n",
    "        four = driver.find_element(By.XPATH, '//span//div//span[@class=\"a-size-small a-color-link a-text-normal\"]').text\n",
    "        return_Exchange.append(four)\n",
    "    except:\n",
    "        return_Exchange.append(\"-\")\n",
    "    \n",
    "\n",
    "    #Deliver date\n",
    "    try:\n",
    "        five = driver.find_element(By.XPATH, '//div[@id=\"mir-layout-DELIVERY_BLOCK-slot-PRIMARY_DELIVERY_MESSAGE_LARGE\"]//span//span[@class=\"a-text-bold\"]').text\n",
    "        expected_Delivery.append(five)\n",
    "    except:\n",
    "        expected_Delivery.append(\"-\")\n",
    "    \n",
    "\n",
    "    #availability\n",
    "    try:\n",
    "        five = driver.find_element(By.XPATH, '//div[@id=\"availability\"]').text\n",
    "        availability.append(five)\n",
    "    except:\n",
    "        availability.append(\"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4a06ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame({\"Brand\":brand_name, \"Product Name\":product_name, \"Price\":price_tag, \"Returnable\":return_Exchange, \"Delivery Date\":expected_Delivery, \"Product URL\":prod_url})\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3de6275",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fa3b84",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb0190b",
   "metadata": {},
   "source": [
    "### 3. Write a python program to access the search bar and search button on images.google.com and scrape 10 images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2cb811",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_img=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e7081c",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "keywords = ['fruits', 'cars', 'Machine Learning', 'Guitar', 'Cakes']\n",
    "\n",
    "for word in keywords:\n",
    "    driver.get('https://images.google.com/')\n",
    "    driver.find_element(By.XPATH, '//textarea[@class=\"gLFyf\"]').send_keys(word)\n",
    "    driver.find_element(By.XPATH,'//div[@class=\"zgAlFc\"]').click()\n",
    "    time.sleep(4)\n",
    "    prod = driver.find_elements(By.XPATH,'//div[@class=\"bRMDJf islir\"]//img')\n",
    "    for i in prod[0:10]:\n",
    "        word_img.append(i.get_attribute('src'))\n",
    "    time.sleep(2)\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcade2bf",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737326ff",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029d042e",
   "metadata": {},
   "source": [
    "### 4. Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on www.flipkart.com and scrape following details for all the search results displayed on 1st page. Details to be scraped: “Brand Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”, “Secondary Camera”, “Display Size”, “Battery Capacity”, “Price”, “Product URL”. \n",
    "Incase if any of the details is missing then replace it by “- “. Save your results in a dataframe and CSV. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f11c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96a2aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_4=[]\n",
    "name_4=[]\n",
    "color_4=[]\n",
    "ram_4=[]\n",
    "rom_4=[]\n",
    "cam_4=[]\n",
    "cam2_4=[]\n",
    "size_4=[]\n",
    "batter_4=[]\n",
    "cost_4=[]\n",
    "prod_url_4=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0766ff6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get('https://www.flipkart.com/')\n",
    "driver.find_element(By.XPATH, '//button[@class=\"_2KpZ6l _2doB4z\"]').click()\n",
    "driver.find_element(By.XPATH, '//input[@class=\"_3704LK\"]').send_keys(\"Oneplus Nord, pixel 4A\")\n",
    "driver.find_element(By.XPATH, '//button[@class=\"L0Z3Pu\"]').click()\n",
    "time.sleep(2)\n",
    "items = driver.find_elements(By.XPATH,'//div[@class=\"_13oc-S\"]')\n",
    "j=0\n",
    "for i in items:\n",
    "    \n",
    "    #brand name\n",
    "    bname=i.find_element(By.CSS_SELECTOR,'._4rR01T')\n",
    "    brand_4.append(bname.text)\n",
    "    \n",
    "    #phone Name\n",
    "    name=i.find_element(By.CSS_SELECTOR,'._4rR01T')\n",
    "    name_4.append(name.text)\n",
    "\n",
    "    #color\n",
    "    col=re.sub(r\".*\\(\" , \"\" , name.text)\n",
    "    col=re.sub(r\"\\,.*\" , \"\" , col)\n",
    "    color_4.append(col)\n",
    "\n",
    "    #ram\n",
    "    ram=i.find_elements(By.XPATH,'//li[@class=\"rgWa7D\"][1]')\n",
    "    temp_ram=re.sub(r\" \\|.*\" , \"\" , ram[j].text)\n",
    "    ram_4.append(temp_ram)\n",
    "    \n",
    "    #rom\n",
    "    rom=re.sub(r\".* \\| \" , \"\" ,ram[j].text)\n",
    "    rom_4.append(rom)\n",
    "    \n",
    "    \n",
    "    #CAMERA\n",
    "    cam=i.find_elements(By.XPATH,'//li[@class=\"rgWa7D\"][3]')\n",
    "    #frontCam\n",
    "    fcam=re.sub(r\" \\|.*\" , \"\" , cam[j].text)\n",
    "    cam_4.append(fcam)\n",
    "    \n",
    "    #secondCam\n",
    "    scam=re.sub(r\".* \\| \", \"\" , cam[j].text)\n",
    "    if fcam!=scam:\n",
    "        cam2_4.append(scam)\n",
    "    else:\n",
    "        cam2_4.append(\"-\")\n",
    "        \n",
    "    #display size\n",
    "    size=i.find_elements(By.XPATH,'//li[@class=\"rgWa7D\"][2]')\n",
    "    size_4.append(size[j].text)\n",
    "    \n",
    "    #battery capacity\n",
    "    bat=i.find_elements(By.XPATH,'//li[@class=\"rgWa7D\"][4]')\n",
    "    batter_4.append(bat[j].text)\n",
    "\n",
    "    #price\n",
    "    cost=i.find_element(By.CSS_SELECTOR,'._30jeq3._1_WHN1')\n",
    "    cost_4.append(cost.text)\n",
    "    j+=1\n",
    "\n",
    "prod_url=driver.find_elements(By.CLASS_NAME,'_1fQZEK')\n",
    "for i in prod_url:\n",
    "    prod_url_4.append(i.get_attribute('href'))\n",
    "    \n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b893e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4=pd.DataFrame({\"Brand\":brand_4, \"Smartphone name\":name_4, \"Colour\":color_4, \"RAM\":ram_4, \"ROM(storage)\":rom_4, \"Primary Camera\":cam_4, \"Secondary Camera\":cam2_4, \"Display Size\":size_4, \"Battery Capacity\":batter_4, \"Price\":cost_4, \"Product URL\":prod_url_4})\n",
    "df4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231bd883",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd87a988",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8133c955",
   "metadata": {},
   "source": [
    "### 5. Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google maps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dec7119",
   "metadata": {},
   "outputs": [],
   "source": [
    "city=str(input(\"Enter the city name for which you want the coordinates of: \"))\n",
    "driver = webdriver.Chrome()\n",
    "driver.get('https://www.google.com/maps/')\n",
    "driver.find_element(By.XPATH,'/html/body/div[3]/div[8]/div[3]/div[1]/div[1]/div/div[2]/form/input').send_keys(city)\n",
    "driver.find_element(By.XPATH,'/html/body/div[3]/div[8]/div[3]/div[1]/div[1]/div/div[2]/div[1]/button').click()\n",
    "\n",
    "time.sleep(3)\n",
    "# We can see that whatever city we search for, its coordinates is stored in the URL\n",
    "url_link=driver.current_url\n",
    "time.sleep(2)\n",
    "#now editing the url so that we get latitude and longitude\n",
    "coordinates=re.sub(r'.*@','',url_link)\n",
    "coordinates=re.match('.*\\,',coordinates).group()\n",
    "\n",
    "driver.quit()\n",
    "#seperating latitude and longitude\n",
    "point_of_break = 0\n",
    "for i in coordinates:\n",
    "    if i!=',':\n",
    "        point_of_break+=1\n",
    "    else:\n",
    "        break\n",
    "latitude=coordinates[0:point_of_break]\n",
    "longitude=coordinates[point_of_break+1:-1]\n",
    "\n",
    "print(\"Latitude = \"+latitude)\n",
    "print(\"Longitude = \"+longitude)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d72981",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c8d844",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70c414f",
   "metadata": {},
   "source": [
    "### 6. Write a program to scrap all the available details of best gaming laptops from digit.in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5230f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86869f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_6=[]\n",
    "driver=webdriver.Chrome()\n",
    "driver.get('https://www.digit.in/')\n",
    "driver.find_element(By.XPATH,'/html/body/div[1]/div/div[3]/div[2]/a/img').click()\n",
    "driver.find_element(By.ID,'globalPageSearchText').send_keys('best gaming laptops')\n",
    "\n",
    "#there is no search button so we have to replicate the \"Enter\" action after writing in the search text field\n",
    "driver.find_element(By.ID,'globalPageSearchText').send_keys(Keys.ENTER)\n",
    "time.sleep(3)\n",
    "products=driver.find_elements(By.XPATH,'//div[@class=\"searchPage\"]')\n",
    "time.sleep(2)\n",
    "j=0\n",
    "for i in products:\n",
    "    prod_url=i.find_elements(By.XPATH,'//div[@class=\"searchPage\"]//a')\n",
    "    product_6.append(prod_url[j].get_attribute('href'))\n",
    "    j+=1\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85175eb",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f353f362",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29e994c",
   "metadata": {},
   "source": [
    "### 7. Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be scrapped: “Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5579ba4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "rank_7=[]\n",
    "name_7=[]\n",
    "net_worth=[]\n",
    "age_7=[]\n",
    "citizenship_7=[]\n",
    "source=[]\n",
    "industry_7=[]\n",
    "\n",
    "\n",
    "\n",
    "driver=webdriver.Chrome()\n",
    "driver.get('https://www.forbes.com/?sh=772c181b2254')\n",
    "driver.maximize_window()\n",
    "time.sleep(2)\n",
    "driver.find_element(By.CSS_SELECTOR,'.hamburger_svg__fs-icon.hamburger_svg__fs-icon--hamburger').click()\n",
    "\n",
    "# we need to hover over an element using actionchains\n",
    "ac=ActionChains(driver)\n",
    "ac.move_to_element(driver.find_element(By.XPATH,'//li[@class=\"TjJgrPSg cD45ib6e primary\"]')).perform()\n",
    "driver.find_element(By.XPATH,'/html/body/div[1]/header/nav/div[1]/div[1]/div/div[2]/ul/li[2]/div[2]/div[3]/ul/li[1]/a').click()\n",
    "\n",
    "# Extracting data\n",
    "#but first we have to click on the first row so that it is collapsed and its class name is matched with every other row\n",
    "while True:\n",
    "    time.sleep(5)\n",
    "    driver.find_element(By.XPATH,'//div[@class=\"Table_rank___YBhk Table_dataCell__2QCve\"]').click()\n",
    "    time.sleep(2)\n",
    "    all_rows = driver.find_elements(By.XPATH,'//div[@class=\"TableRow_cell__db-hv Table_cell__houv9\"]')\n",
    "\n",
    "    # Storing data\n",
    "    for i in all_rows[0::7]:\n",
    "        rank_7.append(i.text)\n",
    "    for j in all_rows[1::7]:\n",
    "        name_7.append(j.text)\n",
    "    for k in all_rows[2::7]:\n",
    "        net_worth.append(k.text)\n",
    "    for l in all_rows[3::7]:\n",
    "        age_7.append(l.text)\n",
    "    for m in all_rows[4::7]:\n",
    "        citizenship_7.append(m.text)\n",
    "    for n in all_rows[5::7]:\n",
    "        source.append(n.text)\n",
    "    for o in all_rows[6::7]:\n",
    "        industry_7.append(o.text)\n",
    "    \n",
    "    try:\n",
    "        driver.find_element(By.XPATH,'//span[text()=\"Next 200\"]').click()\n",
    "    except:\n",
    "        break\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1b98e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df7 = pd.DataFrame({\"Rank\":rank_7, \"Name\":name_7, \"Net worth\":net_worth, \"Age\":age_7, \"Citizenship\":citizenship_7, \"Source\":source, \"Industry\":industry_7})\n",
    "df7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0ae042",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4f10ef",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23754785",
   "metadata": {},
   "source": [
    "### 8. Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted from any YouTube Video. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1825600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method used\n",
    "\n",
    "# 1. in youtube the comments dynamically appears when we scroll down so we want to mimic scroll down\n",
    "# action till we find 500 or less comments and then quit the driver\n",
    "\n",
    "\n",
    "# 2.we will use Keys\n",
    "\n",
    "\n",
    "# 3. we will use script to know the current/old scroll position\n",
    "# https://www.w3schools.com/jsref/prop_win_pageyoffset.asp\n",
    "\n",
    "# in 2 scrolls, approx 5 to 6 new comments are displayed so we will scroll a maximum of 90 times\n",
    "# so that we don't have unnecessary scrapped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829c38d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c5aa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.keys import Keys\n",
    "comment_8=[]\n",
    "upvote_8=[]\n",
    "time_8=[]\n",
    "name=str(input(\"Enter the video link for which you want to scrape the comments\"))\n",
    "driver = webdriver.Chrome()\n",
    "try:\n",
    "    driver.get(name)\n",
    "except:\n",
    "    pass\n",
    "time.sleep(3)\n",
    "old=driver.execute_script('return pageYOffset;')\n",
    "\n",
    "for max_scoll in range(0,95):\n",
    "    print(max_scoll)\n",
    "    driver.find_element(By.TAG_NAME,'body').send_keys(Keys.PAGE_DOWN)\n",
    "    time.sleep(1)\n",
    "    new=driver.execute_script('return pageYOffset;')\n",
    "    if old==new:\n",
    "        break\n",
    "    old=new\n",
    "\n",
    "data = driver.find_elements(By.XPATH,'//ytd-comment-thread-renderer[@class=\"style-scope ytd-item-section-renderer\"]')\n",
    "time.sleep(5)\n",
    "\n",
    "j=0\n",
    "for i in data[0:500]:\n",
    "    comment = i.find_elements(By.XPATH,'//yt-formatted-string[@class=\"style-scope ytd-comment-renderer\"]')\n",
    "    comment_8.append(comment[j].text)\n",
    "    upvote=i.find_elements(By.XPATH,'//span[@id=\"vote-count-middle\"]')\n",
    "    upvote_8.append(upvote[j].text)\n",
    "    time_date=i.find_elements(By.XPATH,'//ytd-comment-thread-renderer//yt-formatted-string[@class=\"published-time-text style-scope ytd-comment-renderer\"]')\n",
    "    time_8.append(time_date[j].text)\n",
    "    j+=1\n",
    "    \n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e863dc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_8 = pd.DataFrame({\"Comment\":comment_8, \"Upvotes\": upvote_8, \"Time of comment\":time_8})\n",
    "df_8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6277fed2",
   "metadata": {},
   "source": [
    "### 9. Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in “London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews, overall reviews, privates from price, dorms from price, facilities and property description. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bc699f",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_9=[]\n",
    "rating_9=[]\n",
    "overall_review_9=[]\n",
    "total_review_9=[]\n",
    "facilities_OR_prop_desc_9=[]\n",
    "distance_9=[]\n",
    "private_cost_9=[]\n",
    "dorm_cost_9 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fdee67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.keys import Keys\n",
    "driver = webdriver.Chrome()\n",
    "driver.get('https://www.hostelworld.com/')\n",
    "driver.maximize_window()\n",
    "driver.find_element(By.XPATH,'//input[@class=\"native-input body-1-reg\"]').click()\n",
    "driver.find_element(By.XPATH,'//input[@class=\"native-input body-1-reg\"]').send_keys(\"London\")\n",
    "time.sleep(5)\n",
    "driver.find_element(By.XPATH,'//button[@class=\"item-content\"]').click()\n",
    "time.sleep(1)\n",
    "driver.find_element(By.XPATH,'//button[@class=\"btn-content large-button icon-only\"]').click()\n",
    "j=0\n",
    "k=0\n",
    "while True:\n",
    "    time.sleep(2)\n",
    "    data=driver.find_elements(By.XPATH,'//div[@class=\"property-card\"]')\n",
    "    time.sleep(2)\n",
    "    \n",
    "#scraping data\n",
    "    j=0\n",
    "    for i in data:\n",
    "        #extracting Name\n",
    "        name=i.find_elements(By.XPATH,'//div[@class=\"property-name\"]')\n",
    "        name_9.append(name[j].text)\n",
    "        j+=1\n",
    "    \n",
    "        #extracting Rating\n",
    "        try:\n",
    "            rating=i.find_elements(By.CLASS_NAME,'number')\n",
    "            rating_9.append(rating[0].text)\n",
    "        except:\n",
    "            rating_9.append('-')\n",
    "            \n",
    "        #extracting overall review\n",
    "        try:\n",
    "            review_rating=i.find_elements(By.CLASS_NAME,'keyword')\n",
    "            overall_review_9.append(review_rating[0].text)\n",
    "        except:\n",
    "            overall_review_9.append('-')\n",
    "            \n",
    "        #extracting total number of review\n",
    "        try:\n",
    "            tot_rev=i.find_elements(By.CLASS_NAME,'left-margin')\n",
    "            total_review_9.append(tot_rev[0].text)\n",
    "        except:\n",
    "            total_review_9.append('-')\n",
    "            \n",
    "        #extracting desc of property\n",
    "        try:\n",
    "            desc=i.find_elements(By.CLASS_NAME,'distance-property-type')\n",
    "            facilities_OR_prop_desc_9.append(desc[0].text)\n",
    "        except:\n",
    "            facilities_OR_prop_desc_9.append('-')\n",
    "            \n",
    "        #extracting the distance\n",
    "        try:\n",
    "            dist=i.find_elements(By.CLASS_NAME,'distance-description')\n",
    "            distance_9.append(dist[0].text)\n",
    "        except:\n",
    "            distance_9.append('-')\n",
    "            \n",
    "        #extracting private cost\n",
    "        try:\n",
    "            pcost=i.find_elements(By.CLASS_NAME,'property-accommodation-price')\n",
    "            private_cost_9.append(pcost[0].find_element(By.CLASS_NAME, 'current').text)\n",
    "        except:\n",
    "            private_cost_9.append('-')\n",
    "        \n",
    "        #extracting dorm cost\n",
    "        try:\n",
    "            pcost=i.find_elements(By.CLASS_NAME,'property-accommodation-price')\n",
    "            dorm_cost_9.append(pcost[1].find_element(By.CLASS_NAME, 'current').text)\n",
    "        except:\n",
    "            dorm_cost_9.append('-')\n",
    "            \n",
    "            \n",
    "    #we need to scroll a little bit to go to next page\n",
    "    for i in range(0,10):\n",
    "        driver.find_element(By.TAG_NAME,'body').send_keys(Keys.PAGE_DOWN)\n",
    "    time.sleep(2)\n",
    "    button_status=driver.find_element(By.XPATH,'//button[@class=\"pill-content page-nav nav-right icon-only\"]')\n",
    "    button_status.get_property('disabled')\n",
    "    if button_status.get_property('disabled')!=True:\n",
    "        driver.find_element(By.CSS_SELECTOR,'.pill-content.page-nav.nav-right.icon-only').click()\n",
    "        time.sleep(2)\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b58e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_9 = pd.DataFrame({\"Hostel Name\":name_9, \"Rating\":rating_9, \"Overall Review Comment\":overall_review_9, \"Total No of Reviews\": total_review_9, \"Property Description\": facilities_OR_prop_desc_9, \"Distance from City\": distance_9, \"Private Room Cost\": private_cost_9, \"Dorm Room Cost\": dorm_cost_9})\n",
    "df_9"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
