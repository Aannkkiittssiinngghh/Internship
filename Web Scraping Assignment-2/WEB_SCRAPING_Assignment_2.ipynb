{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e786cd93",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397b27c8",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1692a1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17987e9f",
   "metadata": {},
   "source": [
    "### Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.shine.com/\n",
    "2. Enter “Data Analyst” in “Job title, Skills” field and enter “Bangalore” in “enter the location” field.\n",
    "3. Then click the searchbutton.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "\n",
    "Note: All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b560bcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title = []\n",
    "job_loc = []\n",
    "company = []\n",
    "exp_req = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664f273b",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://www.shine.com/\")\n",
    "\n",
    "# there is a pop-up approximately after 10 seconds > to close that we use the sleep and click function\n",
    "time.sleep(12)\n",
    "cancel_popup = driver.find_element(By.XPATH, '//button[@class=\"btn-close \"]')\n",
    "cancel_popup.click()\n",
    "\n",
    "\n",
    "# clicking on the search bar\n",
    "driver.find_element(By.CLASS_NAME,'searchBarInput').click()\n",
    "time.sleep(2)\n",
    "\n",
    "# step2\n",
    "skills = driver.find_element(By.XPATH, '/html/body/div[1]/div[4]/div/div[2]/div[2]/div/form/div/div[1]/ul/li[1]/div/input')\n",
    "skills.send_keys('Data Analyst')\n",
    "\n",
    "location = driver.find_element(By.XPATH,'/html/body/div[1]/div[4]/div/div[2]/div[2]/div/form/div/div[1]/ul/li[2]/div/input')\n",
    "location.send_keys('Bangalore')\n",
    "\n",
    "# step3\n",
    "driver.find_element(By.XPATH, '//button[@class=\" btn btn-secondary undefined\"]').click()\n",
    "\n",
    "# There is another Register pop-up approx after 14 to 18 seconds > We need to close that\n",
    "time.sleep(18)\n",
    "driver.find_element(By.XPATH, '//button[@class=\"btn-close register_close\"]').click()\n",
    "time.sleep(2)\n",
    "\n",
    "#Step4\n",
    "name = driver.find_elements(By.XPATH,'//h2[@itemprop=\"name\"]')\n",
    "location = driver.find_elements(By.CSS_SELECTOR, \".jobCard_jobCard_lists_item__YxRkV.jobCard_locationIcon__zrWt2\")\n",
    "comp_name = driver.find_elements(By.XPATH, '//div[@class=\"jobCard_jobCard_cName__mYnow\"]')\n",
    "exp = driver.find_elements(By.XPATH,'//div[@class=\" jobCard_jobCard_lists_item__YxRkV jobCard_jobIcon__3FB1t\"]')\n",
    "\n",
    "for i in name[0:10]:\n",
    "    job_title.append(i.text)\n",
    "\n",
    "    \n",
    "for j in location[0:10]:\n",
    "    extra_text = j.find_element(By.CSS_SELECTOR, \".more_info.ml-10\").text\n",
    "    job_loc.append(j.text.replace(extra_text,\"\").strip())\n",
    "    \n",
    "for k in comp_name[0:10]:\n",
    "    company.append(k.text)\n",
    "    \n",
    "for l in exp[0:10]:\n",
    "    exp_req.append(l.text)\n",
    "    \n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ced40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(job_title), len(job_loc), len(company), len(exp_req))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09933470",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"Job Title\":job_title, \"Location\":job_loc, \"Company Name\":company, \"experience\":exp_req})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027eced5",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cf4522",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e999c13d",
   "metadata": {},
   "source": [
    "### Q2:Write a python program to scrape data for “Data Scientist” Job position in“Bangalore” location. You have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data.\n",
    "\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.shine.com/\n",
    "2. Enter “Data Scientist” in “Job title, Skills” field and enter “Bangalore” in “enter thelocation” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "\n",
    "Note: All of the above steps have to be done in code. No step is to be done manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2900bb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title_2 = []\n",
    "job_loc_2 = []\n",
    "company_2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0316ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://www.shine.com/\")\n",
    "\n",
    "# there is a pop-up approximately after 10 seconds > to close that we use the click function\n",
    "time.sleep(12)\n",
    "cancel_popup = driver.find_element(By.XPATH, '//button[@class=\"btn-close \"]')\n",
    "cancel_popup.click()\n",
    "\n",
    "\n",
    "# clicking on the search bar\n",
    "driver.find_element(By.CLASS_NAME,'searchBarInput').click()\n",
    "time.sleep(2)\n",
    "\n",
    "# step2\n",
    "skills = driver.find_element(By.XPATH, '/html/body/div[1]/div[4]/div/div[2]/div[2]/div/form/div/div[1]/ul/li[1]/div/input')\n",
    "skills.send_keys('Data Scientist')\n",
    "\n",
    "location = driver.find_element(By.XPATH,'/html/body/div[1]/div[4]/div/div[2]/div[2]/div/form/div/div[1]/ul/li[2]/div/input')\n",
    "location.send_keys('Bangalore')\n",
    "\n",
    "# step3\n",
    "driver.find_element(By.XPATH, '//button[@class=\" btn btn-secondary undefined\"]').click()\n",
    "\n",
    "# There is another Register pop-up approx after 14 to 18 seconds > We need to close that\n",
    "time.sleep(18)\n",
    "driver.find_element(By.XPATH, '//button[@class=\"btn-close register_close\"]').click()\n",
    "time.sleep(2)\n",
    "\n",
    "#Step4\n",
    "name = driver.find_elements(By.XPATH,'//h2[@itemprop=\"name\"]')\n",
    "location = driver.find_elements(By.CSS_SELECTOR, \".jobCard_jobCard_lists_item__YxRkV.jobCard_locationIcon__zrWt2\")\n",
    "comp_name = driver.find_elements(By.XPATH, '//div[@class=\"jobCard_jobCard_cName__mYnow\"]')\n",
    "\n",
    "for i in name[0:10]:\n",
    "    job_title_2.append(i.text)\n",
    "\n",
    "    \n",
    "for j in location[0:10]:\n",
    "    extra_text = j.find_element(By.CSS_SELECTOR, \".more_info.ml-10\").text\n",
    "    job_loc_2.append(j.text.replace(extra_text,\"\").strip())\n",
    "    \n",
    "for k in comp_name[0:10]:\n",
    "    company_2.append(k.text)\n",
    "\n",
    "driver.quit()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371566bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(job_title_2), len(job_loc_2), len(company_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1172d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame({\"Job Title\":job_title_2, \"Location\":job_loc_2, \"Company Name\":company_2})\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec6e388",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e47ed3",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a4a8de",
   "metadata": {},
   "source": [
    "### Q3: In this question you have to scrape data using the filters available on the webpage. You have to use the location and salary filter.\n",
    "\n",
    "You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "\n",
    "You have to scrape the job-title, job-location, company name, experience required.\n",
    "\n",
    "The location filter to be used is “Delhi/NCR”. The salary filter to be used is “3-6” lakhs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd5d846",
   "metadata": {},
   "outputs": [],
   "source": [
    "title = []\n",
    "loc_job = []\n",
    "comp_name_3 = []\n",
    "exp_req =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8592cce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get('https://www.shine.com/')\n",
    "\n",
    "# there is a pop-up approximately after 10 seconds > to close that we use the click function\n",
    "time.sleep(12)\n",
    "cancel_popup = driver.find_element(By.XPATH, '//button[@class=\"btn-close \"]')\n",
    "cancel_popup.click()\n",
    "\n",
    "# clicking on the search bar\n",
    "driver.find_element(By.CLASS_NAME,'searchBarInput').click()\n",
    "time.sleep(2)\n",
    "\n",
    "skills = driver.find_element(By.XPATH, '/html/body/div[1]/div[4]/div/div[2]/div[2]/div/form/div/div[1]/ul/li[1]/div/input')\n",
    "skills.send_keys('Data Scientist')\n",
    "\n",
    "driver.find_element(By.XPATH, '//button[@class=\" btn btn-secondary undefined\"]').click()\n",
    "\n",
    "# There is another Register pop-up approx after 14 to 18 seconds > We need to close that\n",
    "time.sleep(18)\n",
    "driver.find_element(By.XPATH, '//button[@class=\"btn-close register_close\"]').click()\n",
    "time.sleep(2)\n",
    "\n",
    "#clicking on the filter option\n",
    "driver.find_element(By.XPATH, '//button[@class=\"filter_filterBtn__FAqUy btn btn-outline-grey font-size-xs \"]').click()\n",
    "time.sleep(1)\n",
    "\n",
    "#clicking on the delhi option\n",
    "driver.find_element(By.XPATH, '/html/body/div[1]/div[1]/div[4]/div/div[1]/div/div[2]/div[2]/div/div/div/div[3]/div/div/div/ul/li[8]/span/label').click()\n",
    "time.sleep(1)\n",
    "\n",
    "#clicking on salary filter option\n",
    "driver.find_element(By.XPATH, '/html/body/div[1]/div[1]/div[4]/div/div[1]/div/div[2]/div[2]/div/div/div/div[3]/div/ul/li[3]').click()\n",
    "time.sleep(1)\n",
    "\n",
    "#THere is no salary filter as 3 to 6 lakh\n",
    "#instead using 3 to 5 lakh filter option and clicking it\n",
    "driver.find_element(By.XPATH, '//label[@for=\"filter_jFSal_1\"]').click()\n",
    "\n",
    "#Clicking on Show Results\n",
    "driver.find_element(By.XPATH,'/html/body/div[1]/div[1]/div[4]/div/div[1]/div/div[2]/div[2]/div/div/div/div[4]/button[2]').click()\n",
    "\n",
    "name = driver.find_elements(By.XPATH,'//h2[@itemprop=\"name\"]')\n",
    "location = driver.find_elements(By.CSS_SELECTOR, \".jobCard_jobCard_lists_item__YxRkV.jobCard_locationIcon__zrWt2\")\n",
    "comp_name = driver.find_elements(By.XPATH, '//div[@class=\"jobCard_jobCard_cName__mYnow\"]')\n",
    "exp = driver.find_elements(By.XPATH,'//div[@class=\" jobCard_jobCard_lists_item__YxRkV jobCard_jobIcon__3FB1t\"]')\n",
    "\n",
    "for i in name[0:10]:\n",
    "    title.append(i.text)\n",
    "\n",
    "    \n",
    "for j in location[0:10]:\n",
    "    extra_text = j.find_element(By.CSS_SELECTOR, \".more_info.ml-10\").text\n",
    "    loc_job.append(j.text.replace(extra_text,\"\").strip())\n",
    "    \n",
    "for k in comp_name[0:10]:\n",
    "    comp_name_3.append(k.text)\n",
    "    \n",
    "for l in exp[0:10]:\n",
    "    exp_req.append(l.text)\n",
    "    \n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67a02d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(title), len(loc_job), len(comp_name_3), len(exp_req))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c26227",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame({\"Job Title\":title, \"Job Location\":loc_job, \"Company Name\":comp_name_3, \"Experience Required\":exp_req})\n",
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9a5cb5",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755b330a",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7d7ded",
   "metadata": {},
   "source": [
    "### Q4: Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes: Brand, ProductDescription, Price and discount off\n",
    "\n",
    "To scrape the data you have to go through following steps:\n",
    "1. Go to Flipkart webpage by url :https://www.flipkart.com/\n",
    "2. Enter “sunglasses” in the search fieldwhere “search for products, brands and more” is written and click the search icon\n",
    "3. After that you will reach to the page having a lot of sunglasses. From this page you can scrap the required data as usual.\n",
    "4. After scraping data from the first page, go to the “Next” Button at the bottom other page , then click on it.\n",
    "5. Now scrape data from this page as usual\n",
    "6. Repeat this until you get data for 100sunglasses.\n",
    "\n",
    "Note: That all of the above steps have to be done by coding only and not manually.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3a543a",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c26b29d",
   "metadata": {},
   "source": [
    " ### ISSUE OBSERVED"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8718ea1f",
   "metadata": {},
   "source": [
    "### It seems like when we open Flipkart it has two types of interfaces maybe to protect the website from bots so, i will be using try and except to navigate the flow of our automated script\n",
    "\n",
    "#### In flow 1/ interface 1 > there is a popup and search button is on the right with different class name > The next page buttons XPATH is also different > code inside Try and else block\n",
    "\n",
    "#### In flow2/interface 2 > there is no popup and the search button is on left > code inside except block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafe984a",
   "metadata": {},
   "outputs": [],
   "source": [
    "brand = []\n",
    "product_desc = []\n",
    "price = []\n",
    "discount = []\n",
    "\n",
    "total_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bc047f",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://www.flipkart.com/\")\n",
    "time.sleep(2)\n",
    "\n",
    "try:\n",
    "    #Flow1\n",
    "    #closing the pop-up\n",
    "    driver.find_element(By.XPATH, '//button[@class=\"_2KpZ6l _2doB4z\"]').click()\n",
    "    time.sleep(1)\n",
    "except:\n",
    "    #Flow2\n",
    "    search = driver.find_element(By.XPATH, '//input[@class=\"Pke_EE\"]')\n",
    "    search.send_keys(\"sunglasses\")\n",
    "    driver.find_element(By.XPATH, '//button[@class=\"_2iLD__\"]').click()\n",
    "    \n",
    "    # By using this while loop we will get approximately 100 rows > a little over it but we will remove extra rows later\n",
    "    \n",
    "    \n",
    "    while total_count<=100:\n",
    "        time.sleep(3)\n",
    "        #flow2\n",
    "        name_brand = driver.find_elements(By.CLASS_NAME, '_2WkVRV')\n",
    "        description = driver.find_elements(By.XPATH, '//a[starts-with(@class,\"IRpwTa\")]')\n",
    "        prod_price = driver.find_elements(By.XPATH, '//div[@class=\"_30jeq3\"]')\n",
    "        discount_perc = driver.find_elements(By.XPATH,'//div[@class=\"_25b18c\"]')\n",
    "        \n",
    "        for i in name_brand:\n",
    "            total_count+=1\n",
    "            brand.append(i.text)\n",
    "\n",
    "        for j in description:\n",
    "            product_desc.append(j.text)\n",
    "\n",
    "        for k in prod_price:\n",
    "            price.append(k.text)\n",
    "            \n",
    "        for i in discount_perc[0:40]:\n",
    "            try:\n",
    "                disc = i.find_element(By.CLASS_NAME,'_3Ay6Sb')\n",
    "                discount.append(disc.text)\n",
    "            except:\n",
    "                discount.append(\"\")    \n",
    "        \n",
    "        if(total_count>100):\n",
    "            break\n",
    "        time.sleep(5)\n",
    "        #searching for next page and clicking it\n",
    "        driver.find_element(By.XPATH, '/html/body/div/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[last()]').click()\n",
    "        time.sleep(4)\n",
    "    driver.quit()\n",
    "    \n",
    "else:\n",
    "    search = driver.find_element(By.XPATH, '//input[@class=\"_3704LK\"]')\n",
    "    search.send_keys(\"sunglasses\")\n",
    "    driver.find_element(By.XPATH, '//button[@class=\"L0Z3Pu\"]').click()\n",
    "    \n",
    "    # By using this while loop we will get approximately 100 rows > a little over it but we will remove extra rows later\n",
    "    #FLOW 1 part 2\n",
    "    \n",
    "    while total_count<=100:\n",
    "        time.sleep(3)\n",
    "        #flow1\n",
    "        name_brand = driver.find_elements(By.CLASS_NAME, '_2WkVRV')\n",
    "        description = driver.find_elements(By.XPATH, '//a[starts-with(@class,\"IRpwTa\")]')\n",
    "        prod_price = driver.find_elements(By.XPATH, '//div[@class=\"_30jeq3\"]')\n",
    "        discount_perc = driver.find_elements(By.XPATH,'//div[@class=\"_25b18c\"]')\n",
    "        \n",
    "        for i in name_brand:\n",
    "            total_count+=1\n",
    "            brand.append(i.text)\n",
    "\n",
    "        for j in description:\n",
    "            product_desc.append(j.text)\n",
    "\n",
    "        for k in prod_price:\n",
    "            price.append(k.text)\n",
    "            \n",
    "            \n",
    "        for i in discount_perc[0:40]:\n",
    "            try:\n",
    "                disc = i.find_element(By.CLASS_NAME,'_3Ay6Sb')\n",
    "                discount.append(disc.text)\n",
    "            except:\n",
    "                discount.append(\"\")\n",
    "            \n",
    "            \n",
    "        if(total_count>100):\n",
    "            break\n",
    "        time.sleep(5)\n",
    "        #searching for next page and clicking it\n",
    "        driver.find_element(By.XPATH, '/html/body/div[1]/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[last()]').click()\n",
    "        time.sleep(4)\n",
    "    driver.quit() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210f7a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(brand), len(product_desc), len(price),len(discount) ,total_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e6c583",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.DataFrame({\"BRAND\":brand, \"product description\":product_desc, \"price\": price, \"Discount\":discount})\n",
    "df4 = df4.head(100)\n",
    "df4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d51909",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095b0bb8",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed19da4f",
   "metadata": {},
   "source": [
    "### Q5: Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link: https://www.flipkart.com/apple-iphone-11-black-64-gb/productreviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART\n",
    "\n",
    "In the above page you have to scrape the tick marked attributes. These are:\n",
    "1. Rating\n",
    "2. Review summary\n",
    "3. Full review\n",
    "\n",
    "You have to scrape this data for first 100reviews.\n",
    "\n",
    "Note: All the steps required during scraping should be done through code only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1930ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rating = []\n",
    "review_summary = []\n",
    "full_review = []\n",
    "total_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90a0bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://www.flipkart.com/apple-iphone-11-black-64-gb/product-reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&market\")\n",
    "time.sleep(2)\n",
    "\n",
    "while True:\n",
    "    rating = driver.find_elements(By.CSS_SELECTOR, '._3LWZlK._1BLPMq, ._3LWZlK._1rdVr6._1BLPMq, ._3LWZlK._32lA32._1BLPMq' )\n",
    "    summary = driver.find_elements(By.XPATH, '//p[@class=\"_2-N8zT\"]')\n",
    "    review = driver.find_elements(By.XPATH, '//div[@class=\"t-ZTKy\"]')\n",
    "\n",
    "    for i in rating:\n",
    "        all_rating.append(i.text)\n",
    "\n",
    "\n",
    "    for j in summary:\n",
    "        review_summary.append(j.text)\n",
    "\n",
    "\n",
    "    for k in review:\n",
    "        total_count+=1\n",
    "        full_review.append(k.text)\n",
    "        \n",
    "        \n",
    "    if total_count>99:\n",
    "        break\n",
    "    else:\n",
    "        driver.find_element(By.XPATH, \"//a[span[text()='Next']]\").click()\n",
    "        time.sleep(2)\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a475b9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_rating), len(review_summary), len(full_review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f310d50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = pd.DataFrame({\"Review Summary\":review_summary, \"Rating\":all_rating, \"Review\":full_review})\n",
    "df5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8ce18d",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e213b00",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d913d7e",
   "metadata": {},
   "source": [
    "### Q6: Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the search field.\n",
    "\n",
    "You have to scrape 3 attributes of each sneaker:\n",
    "1. Brand\n",
    "2. ProductDescription\n",
    "3. Price\n",
    "\n",
    "As shown in the below image, you have to scrape the above attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c89847",
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_name_6 = []\n",
    "product_desc_6 = []\n",
    "price_6 = []\n",
    "\n",
    "total_count_6 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f3a58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://www.flipkart.com/\")\n",
    "time.sleep(2)\n",
    "\n",
    "\n",
    "# USING THE SAME LOGIC FROM QUESTION 4 TO TACKLE THE PROBLEM OF TWO DIFFERENT INTERFACES\n",
    "\n",
    "try:\n",
    "    #Flow1\n",
    "    #closing the pop-up\n",
    "    driver.find_element(By.XPATH, '//button[@class=\"_2KpZ6l _2doB4z\"]').click()\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "except:\n",
    "    #Flow2\n",
    "    search = driver.find_element(By.XPATH, '//input[@class=\"Pke_EE\"]')\n",
    "    search.send_keys(\"Sneakers\")\n",
    "    driver.find_element(By.XPATH, '//button[@class=\"_2iLD__\"]').click()\n",
    "    \n",
    "    while True:\n",
    "        name_brand = driver.find_elements(By.CLASS_NAME, '_2WkVRV')\n",
    "        description = driver.find_elements(By.XPATH, '//a[starts-with(@class,\"IRpwTa\")]')\n",
    "        prod_price = driver.find_elements(By.XPATH, '//div[@class=\"_30jeq3\"]')\n",
    "        for i in name_brand:\n",
    "            brand_name_6.append(i.text)\n",
    "\n",
    "        for j in description:\n",
    "            product_desc_6.append(j.text)\n",
    "\n",
    "        for k in prod_price:\n",
    "            price_6.append(k.text)\n",
    "            total_count_6+=1\n",
    "            \n",
    "        if total_count_6>99:\n",
    "            break\n",
    "        else:\n",
    "            driver.find_element(By.XPATH, \"//a[span[text()='Next']]\").click()\n",
    "            time.sleep(3)\n",
    "            \n",
    "    driver.quit()\n",
    "    \n",
    "else:\n",
    "    search = driver.find_element(By.XPATH, '//input[@class=\"_3704LK\"]')\n",
    "    search.send_keys(\"Sneakers\")\n",
    "    driver.find_element(By.XPATH, '//button[@class=\"L0Z3Pu\"]').click()\n",
    "    while True:\n",
    "        name_brand = driver.find_elements(By.CLASS_NAME, '_2WkVRV')\n",
    "        description = driver.find_elements(By.XPATH, '//a[starts-with(@class,\"IRpwTa\")]')\n",
    "        prod_price = driver.find_elements(By.XPATH, '//div[@class=\"_30jeq3\"]')\n",
    "\n",
    "        for i in name_brand:\n",
    "            brand_name_6.append(i.text)\n",
    "\n",
    "        for j in description:\n",
    "            product_desc_6.append(j.text)\n",
    "\n",
    "        for k in prod_price:\n",
    "            price_6.append(k.text)\n",
    "            total_count_6+=1\n",
    "            \n",
    "        time.sleep(2)\n",
    "        if total_count_6>99:\n",
    "                break\n",
    "        else:\n",
    "            driver.find_element(By.XPATH, \"//a[span[text()='Next']]\").click()\n",
    "            time.sleep(3)\n",
    "            \n",
    "            \n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf341b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(brand_name_6), len(product_desc_6), len(price_6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1c4c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = pd.DataFrame({\"Brand\":brand_name_6, \"Price\":price_6, \"Description\":product_desc_6})\n",
    "df6 = df6.head(100)\n",
    "df6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786f3e03",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce767b5",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37269c52",
   "metadata": {},
   "source": [
    "### Q7: Go to webpage https://www.amazon.in/ Enter “Laptop” in the search field and then click the search icon. Then set CPU Type filter to “Intel Core i7” as shown in the below image:\n",
    "After setting the filters scrape first 10 laptops data. You have to scrape 3 attributes for each laptop:\n",
    "1. Title\n",
    "2. Ratings\n",
    "3. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb16bef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_7 = []\n",
    "rating_7 = []\n",
    "price_7 = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13f64a5",
   "metadata": {},
   "source": [
    "#### Fetching whole section/division/element having the data we want to scrape and then fetching data from that all at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b5d85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get('https://www.amazon.in/')\n",
    "search = driver.find_element(By.ID, 'twotabsearchtextbox')\n",
    "search.send_keys(\"Laptop\")\n",
    "driver.find_element(By.ID, 'nav-search-submit-button').click()\n",
    "time.sleep(2)\n",
    "driver.find_element(By.XPATH, \"//span[a[span[text() = 'Intel Core i7']]]\").click()\n",
    "sec = driver.find_elements(By.XPATH, '//div[@class=\"sg-col sg-col-4-of-12 sg-col-8-of-16 sg-col-12-of-20 sg-col-12-of-24 s-list-col-right\"]')\n",
    "time.sleep(2)\n",
    "\n",
    "for data in sec[0:10]:\n",
    "    #fetching title\n",
    "    titles = data.find_elements(By.CSS_SELECTOR, '.a-size-medium.a-color-base.a-text-normal')\n",
    "    title_7.append(titles[0].text)\n",
    "    \n",
    "    #fetching ratings\n",
    "    ratings = data.find_elements(By.CSS_SELECTOR,\".a-size-base.puis-normal-weight-text\")\n",
    "    try:\n",
    "        rating_7.append(ratings[0].text)\n",
    "    except:\n",
    "        rating_7.append(\"Not Rated\")\n",
    "    \n",
    "    #fetching price\n",
    "    prices = data.find_elements(By.CSS_SELECTOR, \".a-price-whole\")\n",
    "    price_7.append(prices[0].text)\n",
    "    \n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3889210",
   "metadata": {},
   "outputs": [],
   "source": [
    "df7 = pd.DataFrame({\"Title\":title_7,\"Rating\":rating_7,\"Cost\":price_7})\n",
    "df7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62135792",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfaf823a",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbc12cf",
   "metadata": {},
   "source": [
    "### Q8: Write a python program to scrape data for Top 1000 Quotes of All Time.\n",
    "The above task will be done in following steps:\n",
    "1. First get the webpagehttps://www.azquotes.com/\n",
    "2. Click on TopQuotes\n",
    "3. Than scrap a) Quote b) Author c) Type Of Quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086bad30",
   "metadata": {},
   "outputs": [],
   "source": [
    "quote_8 = []\n",
    "author_8 = []\n",
    "type_of_quote = []\n",
    "\n",
    "count_8 = 0\n",
    "iterate_over_quotes = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040a623b",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://www.azquotes.com/\")\n",
    "driver.maximize_window()\n",
    "driver.find_element(By.XPATH, '/html/body/div[1]/div[1]/div[1]/div/div[3]/ul/li[5]/a').click()\n",
    "time.sleep(1)\n",
    "while count_8<1000:\n",
    "    Data = driver.find_elements(By.XPATH,'//div[@class=\"wrap-block\"]')\n",
    "    for i in Data:\n",
    "        author = i.find_elements(By.CSS_SELECTOR, '.author')\n",
    "        author_8.append(author[0].text)\n",
    "\n",
    "        type_quote = i.find_elements(By.CSS_SELECTOR, '.tags')\n",
    "        type_of_quote.append(type_quote[0].text)\n",
    "\n",
    "\n",
    "        quote = i.find_elements(By.XPATH,'//a[@class=\"title\"]')\n",
    "        quote_8.append(quote[iterate_over_quotes].text)\n",
    "        iterate_over_quotes+=1\n",
    "        \n",
    "        count_8+=1\n",
    "    iterate_over_quotes=0\n",
    "    try:\n",
    "        driver.find_element(By.XPATH,'//li[@class=\"next\"]').click()\n",
    "        time.sleep(2)\n",
    "    except:\n",
    "        pass\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d75933",
   "metadata": {},
   "outputs": [],
   "source": [
    "df8 = pd.DataFrame({\"Author\":author_8,\"Type\":type_of_quote,\"Quote\":quote_8})\n",
    "df8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e0e8f5",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb4083e",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fd73eb",
   "metadata": {},
   "source": [
    "### Q9: Write a python program to display list of respected former Prime Ministers of India(i.e. Name, Born-Dead, Term of office, Remarks) from https://www.jagranjosh.com/.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.jagranjosh.com/\n",
    "2. Then You have to click on the GK option\n",
    "3. Then click on the List of all Prime Ministers of India\n",
    "4. Then scrap the mentioned data and make the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3762c38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = []\n",
    "dob = []\n",
    "time_in_office = []\n",
    "remark = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58318198",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://www.jagranjosh.com/\")\n",
    "driver.maximize_window()\n",
    "driver.find_element(By.XPATH, '/html/body/div/header/nav/div/div/div[3]/ul/li[3]/a').click()\n",
    "time.sleep(5)\n",
    "\n",
    "#accepting the cookies\n",
    "driver.find_element(By.XPATH, '/html/body/div[1]/div/div/div[3]/div/div[2]/div/div[3]/div/a[3]').click()\n",
    "time.sleep(2)\n",
    "driver.find_element(By.XPATH, '/html/body/div[1]/div/div/div[2]/div/div[10]/div/div/ul/li[2]/a').click()\n",
    "\n",
    "#extracting table\n",
    "table_data = driver.find_element(By.XPATH, '//div[@class=\"table-box\"]')\n",
    "\n",
    "#storing each row\n",
    "row_data = table_data.find_elements(By.TAG_NAME,'tr')\n",
    "\n",
    "\n",
    "#extracting each cell value from each row\n",
    "for i in row_data[1:]:\n",
    "    cell_data = i.find_elements(By.TAG_NAME,'td')\n",
    "    name.append(cell_data[1].text)\n",
    "    dob.append(cell_data[2].text)\n",
    "    time_in_office.append(cell_data[3].text)\n",
    "    remark.append(cell_data[4].text)\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1046e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df9 = pd.DataFrame({\"PM Name\":name,\"Born-Dead\":dob,\"Term of office\":time_in_office,\"Remark\":remark})\n",
    "df9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ce9c40",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72b52a5",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fb1e18",
   "metadata": {},
   "source": [
    "### Q10: Write a python program to display list of 50 Most expensive cars in the world (i.e. Car name and Price) from https://www.motor1.com/\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.motor1.com/\n",
    "2. Then You have to type in the search bar ’50 most expensive cars’\n",
    "3. Then click on 50 most expensive carsin the world..\n",
    "4. Then scrap the mentioned data and make the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bc1adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_10 = []\n",
    "price_10 = []\n",
    "desc_10 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f436221",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://www.motor1.com/\")\n",
    "driver.maximize_window()\n",
    "driver.find_element(By.XPATH,'//input[@class=\"m1-search-panel-input m1-search-form-text\"]').send_keys(\"50 most expensive cars\")\n",
    "driver.find_element(By.XPATH,'//button[@class=\"m1-search-panel-button m1-search-form-button-animate icon-search-svg\"]').click()\n",
    "driver.find_element(By.XPATH,'//h3[a[text()=\"50 Most Expensive Cars In The World\"]]').click()\n",
    "time.sleep(2)\n",
    "\n",
    "for i in range(50):\n",
    "    title = driver.find_elements(By.XPATH,'//h3[@class=\"subheader\"]')\n",
    "    name_10.append(title[i].text)\n",
    "    \n",
    "    \n",
    "    cost = driver.find_elements(By.XPATH, '//p[strong]')\n",
    "    temp_cost = cost[i].text.replace('Price: ',\"\")\n",
    "    temp_cost = temp_cost.replace('$',\"\")\n",
    "    temp_cost = temp_cost.replace(\" (est.)\",\"\")\n",
    "    price_10.append(temp_cost)\n",
    "\n",
    "# there is no way to extract description using other way so i am using absolute path and index value of the paragraph tag<p> since\n",
    "# since it seems from index 5, at the distance of 2, the para tag contains the description tag\n",
    "index_val = 5\n",
    "for i in range(50):\n",
    "    aa = driver.find_element(By.XPATH,f'/html/body/div[10]/div[7]/div[2]/div[1]/div[2]/div[2]/p[{index_val}]')\n",
    "    index_val+=2\n",
    "    desc_10.append(aa.text)\n",
    "    \n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f510ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "df10 = pd.DataFrame({\"Car\":name_10, \"Price\":price_10, \"Car Description\":desc_10})\n",
    "df10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
